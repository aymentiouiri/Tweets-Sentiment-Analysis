{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhjGiT_VRRlB",
        "outputId": "5a501ea3-98f2-423a-da2b-99d88566e2ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0oev9INWQ4uE"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEm3jHdpR7R5",
        "outputId": "baec3fd4-57cb-4f90-edc5-d2859ed09dcf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpTFoEKQ4uG",
        "outputId": "2845e0a9-abaa-4459-a213-0b4ee546383d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/.DS_Store\n",
            "/content/drive/MyDrive/data/tweet-hatred-speech/test.csv\n",
            "/content/drive/MyDrive/data/tweet-hatred-speech/train.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qDCQ18B4Q4uG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data/tweet-hatred-speech/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/tweet-hatred-speech/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5LCImKNDQ4uH"
      },
      "outputs": [],
      "source": [
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uA3zGn8iQ4uH"
      },
      "outputs": [],
      "source": [
        "def processed_tweets(tweet, tokenizer, max_length=128):\n",
        "    return tokenizer.encode_plus(\n",
        "        tweet,\n",
        "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
        "        max_length=max_length,    # Pad & truncate all sentences\n",
        "        padding='max_length',     # Pad to max_length\n",
        "        truncation=True,          # Truncate to max_length\n",
        "        return_attention_mask=True,  # Construct attn. masks\n",
        "        return_tensors='pt',      # Return pytorch tensors\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8YgQgS5sQ4uH"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Upsample class 1 in the training set\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(train_df['tweet'].values.reshape(-1, 1), train_df['label'])\n",
        "train_resampled_df = pd.DataFrame({'tweet': X_train_resampled.flatten(), 'label': y_train_resampled})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAmIkkZzQ4uH",
        "outputId": "789b6cc6-273d-4f5b-c2c8-448ec35a2b1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    26736\n",
              "1    26736\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "y_train_resampled.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "BfH__xo9Q4uI"
      },
      "outputs": [],
      "source": [
        "class TweetsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.df.iloc[idx]['tweet']\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "        encoded_tweet = processed_tweets(tweet, self.tokenizer, self.max_length)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_tweet['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded_tweet['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "VI1ai_QEQ4uI"
      },
      "outputs": [],
      "source": [
        "train_dataset = TweetsDataset(train_resampled_df, tokenizer)\n",
        "val_dataset = TweetsDataset(val_df, tokenizer)\n",
        "\n",
        "# Define data loaders for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4hkiuUKQ4uI",
        "outputId": "814c19ae-e6d7-4dc9-df60-e304a58ab2d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.TweetsDataset at 0x7ced48e0f880>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGBA6LuGQ4uI",
        "outputId": "b8aed05a-761f-4df1-a6b2-1a24501969ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT model for sequence classification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "a_0EaSq2Q4uJ"
      },
      "outputs": [],
      "source": [
        "def apply_lora_with_peft(model, rank=32, alpha=32, dropout=0.05):\n",
        "    lora_config = LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=alpha,\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        lora_dropout=dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "    )\n",
        "    return get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "OWK6E0CtQ4uJ"
      },
      "outputs": [],
      "source": [
        "def fine_tune_bert(train_loader, val_loader, model, epochs, tokenizer):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = apply_lora_with_peft(model).to(device)\n",
        "\n",
        "    # Define optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Define mixed-precision training scaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1} has started')\n",
        "        total_loss = 0.0\n",
        "        total_predictions = []\n",
        "        total_labels = []\n",
        "        model.train()\n",
        "\n",
        "        # Training loop\n",
        "        for batch in train_loader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with autocast():\n",
        "                outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            # Backward pass with mixed precision\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Compute training accuracy\n",
        "            logits = outputs.logits.detach().cpu().numpy()\n",
        "            predictions = torch.argmax(torch.from_numpy(logits), axis=1).numpy()\n",
        "            total_predictions.extend(predictions)\n",
        "            total_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_accuracy = accuracy_score(total_labels, total_predictions)\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1} has ended. Average Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_predictions = []\n",
        "        val_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                with autocast():\n",
        "                    outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "\n",
        "                logits = outputs.logits.detach().cpu().numpy()\n",
        "                predictions = torch.argmax(torch.from_numpy(logits), axis=1).numpy()\n",
        "                val_predictions.extend(predictions)\n",
        "                val_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "        print(f'Validation Accuracy: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkP1SUs4Q4uJ",
        "outputId": "61346a0c-24c1-4661-de6a-3af1665e1f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtRUHkJsQ4uK",
        "outputId": "234ed586-c6af-47df-b7b2-f9fd285e0352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 has started\n",
            "Epoch 1 has ended. Average Loss: 0.5220, Training Accuracy: 0.7216\n",
            "Validation Accuracy: 0.8646\n",
            "Epoch 2 has started\n",
            "Epoch 2 has ended. Average Loss: 0.3162, Training Accuracy: 0.8679\n",
            "Validation Accuracy: 0.8708\n",
            "Epoch 3 has started\n",
            "Epoch 3 has ended. Average Loss: 0.2832, Training Accuracy: 0.8829\n",
            "Validation Accuracy: 0.8905\n",
            "Epoch 4 has started\n",
            "Epoch 4 has ended. Average Loss: 0.2650, Training Accuracy: 0.8930\n",
            "Validation Accuracy: 0.8899\n",
            "Epoch 5 has started\n",
            "Epoch 5 has ended. Average Loss: 0.2512, Training Accuracy: 0.8963\n",
            "Validation Accuracy: 0.8908\n",
            "Epoch 6 has started\n",
            "Epoch 6 has ended. Average Loss: 0.2410, Training Accuracy: 0.9023\n",
            "Validation Accuracy: 0.8865\n",
            "Epoch 7 has started\n",
            "Epoch 7 has ended. Average Loss: 0.2396, Training Accuracy: 0.9036\n",
            "Validation Accuracy: 0.8908\n"
          ]
        }
      ],
      "source": [
        "epochs = 7\n",
        "\n",
        "# Fine-tune BERT\n",
        "fine_tune_bert(train_loader, val_loader, model, epochs, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}